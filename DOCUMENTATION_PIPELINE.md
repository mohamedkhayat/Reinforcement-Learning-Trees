# ğŸ“š Documentation DÃ©taillÃ©e du Pipeline RLT

**Projet:** RÃ©implÃ©mentation et AmÃ©lioration de l'algorithme RLT (Reinforcement Learning Trees)  
**Date:** Novembre 2025  
**Auteurs:** Kousay Najar, Hamza Farhani, Taoufik Krid, Wiem Ben M'Sahel, Rawen Mezzi, Mohamed Khayat

---

## ğŸ¯ Vue d'Ensemble du Projet

Ce projet implÃ©mente l'algorithme **Reinforcement Learning Trees (RLT)**, une mÃ©thode d'ensemble basÃ©e sur les arbres de dÃ©cision, conÃ§ue pour gÃ©rer efficacement les donnÃ©es **haute dimension avec sparsitÃ©** (p >> p1, oÃ¹ p1 = nombre de variables signaux).

### Objectifs Principaux (DSOs)

1. **DSO1:** ImplÃ©menter l'algorithme RLT avec ses 3 stratÃ©gies innovantes
2. **DSO2:** Comparer RLT avec les mÃ©thodes classiques (RF, GBM, XGBoost)
3. **DSO3:** Rendre les dÃ©cisions explicables (XAI)
4. **DSO4:** Optimiser l'algorithme RLT

---

## ğŸ“ Architecture du Projet

```
Reinforcement-Learning-Trees/
â”‚
â”œâ”€â”€ datasets/                          # DonnÃ©es brutes (CSV)
â”‚   â”œâ”€â”€ breast_cancer.csv
â”‚   â”œâ”€â”€ concrete_data.csv
â”‚   â”œâ”€â”€ Parkinsson disease.csv
â”‚   â””â”€â”€ ... (7 autres datasets)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ dataset_wrapper.py        # â­ Classe pour standardiser les datasets
â”‚   â”‚
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ data_understanding.py      # â­ Exploration et visualisation
â”‚       â””â”€â”€ data_preparation.py        # â­ Nettoyage et prÃ©paration
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ notebook.ipynb                 # Pipeline complet (Phases 1-6)
â”‚
â””â”€â”€ DOCUMENTATION_PIPELINE.md          # ğŸ“– Ce fichier
```

---

## ğŸ”§ Composants Python DÃ©taillÃ©s

### 1ï¸âƒ£ `dataset_wrapper.py` - Le Gestionnaire Universel

#### ğŸ¯ **RÃ´le et UtilitÃ©**

Ce fichier rÃ©sout un **problÃ¨me critique** : nous travaillons avec **10 datasets diffÃ©rents** provenant de sources variÃ©es (UCI ML Repository). Chaque dataset a :

- Des **noms de colonnes diffÃ©rents** (ex: "diagnosis" vs "status" vs "Label")
- Des **types de cibles diffÃ©rents** (classification binaire, rÃ©gression, classification multi-classes)
- Des **colonnes d'ID** parfois prÃ©sentes, parfois absentes
- Des **valeurs manquantes** encodÃ©es diffÃ©remment (`?`, `NaN`, `nan`, etc.)

**Solution :** `DatasetWrapper` **unifie** tous ces datasets dans une interface commune.

#### ğŸ“‹ **Structure de DonnÃ©es - `datasets_dict`**

```python
datasets_dict = {
    "breast_cancer": {
        "path": "breast_cancer.csv",          # Chemin relatif du fichier
        "target": "diagnosis",                 # Nom de la colonne cible
        "id_col": "id",                        # Colonne d'identifiant (Ã  exclure)
        "type": "Categorical",                 # Type de tÃ¢che
    },
    "concrete": {
        "path": "concrete_data.csv",
        "target": "Strength",
        "type": "Continuous",                  # Pas d'id_col ici
    },
    # ... 8 autres datasets
}
```

**Points clÃ©s :**

- `target` : identifie la variable Y Ã  prÃ©dire
- `id_col` : optionnel, pour exclure les colonnes non-informatives
- `type` : `"Categorical"` (classification) ou `"Continuous"` (rÃ©gression)

#### ğŸ” **Fonctionnement de la Classe `DatasetWrapper`**

```python
class DatasetWrapper:
    def __init__(self, name):
        # 1. RÃ©cupÃ©rer la configuration du dataset
        config = datasets_dict[name]

        # 2. Charger le CSV avec gestion intelligente des valeurs manquantes
        self.df = pd.read_csv(
            "datasets/" + config["path"],
            na_values=["?", "nan", "NaN", ""]  # Unification
        )

        # 3. Supprimer les duplicatas
        self.df = self.df.drop_duplicates()

        # 4. Identifier automatiquement les variables quantitatives
        all_numerics = self.df.select_dtypes(include=[np.number]).columns.tolist()
        cols_to_exclude = [self.target]
        if self.id_col:
            cols_to_exclude.append(self.id_col)

        self.quantitatives_variables = [
            c for c in all_numerics if c not in cols_to_exclude
        ]

        # 5. Identifier les variables catÃ©gorielles (par dÃ©duction)
        all_columns = self.df.columns.tolist()
        self.categorical_variables = [
            c for c in all_columns
            if (c not in self.quantitatives_variables and c not in cols_to_exclude)
        ]
```

#### âœ… **Avantages de cette Architecture**

| ProblÃ¨me                         | Solution DatasetWrapper                                  |
| -------------------------------- | -------------------------------------------------------- |
| **10 datasets diffÃ©rents**       | Interface unifiÃ©e : `wrapper.df`, `wrapper.target`, etc. |
| **Types de variables variÃ©s**    | Classification automatique en quantitatif/catÃ©goriel     |
| **Valeurs manquantes multiples** | Normalisation lors du chargement (`na_values=...`)       |
| **Colonnes ID parasites**        | Exclusion automatique via `id_col`                       |
| **Code dupliquÃ©**                | Un seul code pour tous les datasets                      |

#### ğŸ”„ **Utilisation Pratique**

```python
# Au lieu de :
# df1 = pd.read_csv("breast_cancer.csv")
# target1 = "diagnosis"
# X1 = df1.drop(columns=["id", "diagnosis"])
# df2 = pd.read_csv("concrete.csv")
# target2 = "Strength"
# X2 = df2.drop(columns=["Strength"])
# ... rÃ©pÃ©ter 10 fois

# Avec DatasetWrapper :
wrapper = DatasetWrapper("breast_cancer")
# wrapper.df         â†’ DataFrame nettoyÃ©
# wrapper.target     â†’ "diagnosis"
# wrapper.quantitatives_variables â†’ ['radius_mean', 'texture_mean', ...]
# wrapper.categorical_variables   â†’ []
```

**Impact :** Code **10x plus court** et **maintenable**.

---

### 2ï¸âƒ£ `data_understanding.py` - L'Explorateur Visuel

#### ğŸ¯ **RÃ´le et UtilitÃ©**

Ce script implÃ©mente la **Phase 2 du CRISP-DM** (Data Understanding). Il gÃ©nÃ¨re automatiquement un **rapport d'analyse exploratoire complet** pour chaque dataset.

#### ğŸ“Š **Fonction Principale : `understand_data(wrapper)`**

**EntrÃ©e :** Un objet `DatasetWrapper`  
**Sortie :** Affichage de 7 analyses + visualisations

#### ğŸ”¬ **Les 7 Ã‰tapes d'Analyse**

##### **1. Statistiques Descriptives Basiques**

```python
print(f"Number of Rows: {len(df)}")           # Ex: 569 (breast_cancer)
print(f"Number of Columns: {df.shape[1]}")   # Ex: 31 colonnes
```

**UtilitÃ© :** Comprendre la taille du dataset (important pour RLT : n^(1/3) = nmin)

##### **2. Classification des Variables**

```python
print("Qualitatives Columns:")
for col in categorical_variables:
    print(col)  # Ex: aucune pour breast_cancer

print("Quantitatives Columns:")
for col in quantitatives_variables:
    print(col)  # Ex: radius_mean, texture_mean, ...
```

**UtilitÃ© :** RLT ne traite que les variables numÃ©riques (pour l'instant)

##### **3. Identification de la Cible**

```python
print(f"Target: {target_variable} (Type: {type_target})")
# Ex: diagnosis (Type: Categorical)
```

**UtilitÃ© :** DÃ©termine si on fait de la classification ou rÃ©gression

##### **4. DÃ©tection des Valeurs Manquantes**

```python
missing_pct = (df.isna().sum() / len(df)) * 100
for name, val in zip(missing_pct.index, missing_pct):
    if val != 0:
        print(f"{name}: {val:.2f}% missing")
```

**UtilitÃ© :** DÃ©cider de la stratÃ©gie d'imputation (KNN dans data_preparation)

##### **5. DÃ©tection des Duplicatas**

```python
duplicated_pct = (df.duplicated().sum() / len(df)) * 100
print(f"{duplicated_pct:.2f}% duplicated rows")
```

**UtilitÃ© :** Les duplicatas sont dÃ©jÃ  supprimÃ©s dans `DatasetWrapper`

##### **6. DÃ©tection des Outliers (MÃ©thode IQR)**

```python
def has_outliers_iqr(data_column):
    q1 = data_column.quantile(0.25)
    q3 = data_column.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = (data_column < lower_bound) | (data_column > upper_bound)
    return outliers.any()
```

**Visualisation :** Boxplots pour chaque variable avec outliers  
**UtilitÃ© :** RLT est **robuste aux outliers** (arbres de dÃ©cision), mais bon Ã  documenter

##### **7. Visualisations ComplÃ¨tes**

**a) Histogrammes (Variables Quantitatives)**

```python
def plot_quantitative_histogram(df, col, bins=30):
    sns.histplot(data=df, x=col, bins=bins, kde=True, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.show()
```

**UtilitÃ© :** Voir si distributions sont normales, bimodales, etc.

**b) Countplots (Variables CatÃ©gorielles + Cible)**

```python
def plot_qualitative_countplot(df, col, top_n=10):
    category_counts = df[col].value_counts().nlargest(top_n)
    sns.countplot(data=df, y=col, order=category_counts.index)
    plt.show()
```

**UtilitÃ© :** DÃ©tecter les **dÃ©sÃ©quilibres de classes** (important pour classification)

**c) Pairplot (Relations 2D)**

```python
def plot_pairplot(df, quant_vars, target, max_features=10):
    cols = quant_vars[:max_features] + [target]
    sns.pairplot(df[cols], hue=target, diag_kind='kde')
    plt.show()
```

**UtilitÃ© :** Visualiser les **corrÃ©lations** et **sÃ©parabilitÃ©** des classes

#### ğŸ¨ **Exemple de Sortie (Breast Cancer)**

```
================================================================================
ğŸ“Œ ANALYZING DATASET: BREAST_CANCER
================================================================================

Number of Rows: 569
Number of Columns: 31

Qualitatives Columns: None
Quantitatives Columns: 30 variables (radius_mean, texture_mean, ...)

Target: diagnosis (Type: Categorical)

No missing values

No duplicated values

Outliers detected in 15 features: [radius_mean, area_mean, ...]
[ğŸ“Š Boxplots affichÃ©s]

[ğŸ“ˆ Histogrammes pour 5 premiÃ¨res variables]
[ğŸ“Š Countplot pour la cible : M=212, B=357]
[ğŸ”— Pairplot des 10 premiÃ¨res variables colorÃ©es par diagnosis]
```

#### ğŸš€ **Optimisations ImplÃ©mentÃ©es**

1. **Limitation des graphiques :**

   - Max 10 outliers boxplots (Ã©viter surcharge)
   - Max 5 histogrammes
   - Max 10 variables dans pairplot

2. **Gestion des erreurs :**

   - `try/except` pour pairplot (peut crasher avec trop de catÃ©gories)
   - VÃ©rification `dropna()` avant calcul IQR

3. **Performance :**
   - `plt.close('all')` pour libÃ©rer mÃ©moire
   - Pas de calculs inutiles sur colonnes vides

---

### 3ï¸âƒ£ `data_preparation.py` - Le Nettoyeur Intelligent

#### ğŸ¯ **RÃ´le et UtilitÃ©**

Ce script implÃ©mente la **Phase 3 du CRISP-DM** (Data Preparation). Il transforme les donnÃ©es **brutes** en donnÃ©es **prÃªtes pour l'entraÃ®nement**.

#### ğŸ”§ **Fonction Principale : `prepare_data(wrapper)`**

**Pipeline en 6 Ã‰tapes :**

```
DonnÃ©es brutes (wrapper.df)
        â†“
[1] Extraction X, y
        â†“
[2] Suppression colonnes > 60% missing
        â†“
[3] Suppression lignes > 50% missing
        â†“
[4] Split Train/Test (80/20)
        â†“
[5] Imputation KNN (k=5)
        â†“
[6] Standardisation (Î¼=0, Ïƒ=1)
        â†“
X_train, X_test, y_train, y_test
```

#### ğŸ“‹ **DÃ©tail des Ã‰tapes**

##### **Ã‰tape 1 : Extraction X et y**

```python
X = df[wrapper.quantitatives_variables].copy()  # Features
y = df[wrapper.target].copy()                   # Target
```

**Pourquoi `.copy()` ?** Ã‰viter les `SettingWithCopyWarning` de pandas

##### **Ã‰tape 2 : Suppression des Colonnes Trop Manquantes**

```python
missing_pct = X.isnull().mean() * 100
cols_to_drop = missing_pct[missing_pct > 60].index
X_clean = X.drop(columns=cols_to_drop)
```

**Seuil :** 60% de valeurs manquantes  
**Justification :** Une colonne avec >60% de NaN apporte peu d'information

##### **Ã‰tape 3 : Suppression des Lignes Trop Manquantes**

```python
row_missing_pct = X_clean.isnull().mean(axis=1)
X_clean = X_clean[row_missing_pct <= 0.5]
y_clean = y.loc[X_clean.index]  # âš ï¸ Synchroniser y avec X
```

**Seuil :** 50% de valeurs manquantes  
**Important :** Toujours synchroniser `y` avec les lignes conservÃ©es de `X`

##### **Ã‰tape 4 : Split Train/Test StratifiÃ©**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y_clean,
    test_size=0.2,      # 80% train, 20% test
    shuffle=True,       # MÃ©lange alÃ©atoire
    random_state=42     # ReproductibilitÃ©
)
```

**Pourquoi 80/20 ?** Standard pour datasets de taille moyenne (n=200-1000)  
**Note :** Pour RLT, on pourrait ajouter `stratify=y_clean` pour classification

##### **Ã‰tape 5 : Imputation par K-Nearest Neighbors**

```python
imputer = KNNImputer(n_neighbors=5)
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)  # âš ï¸ Pas de fit sur test !
```

**Pourquoi KNN plutÃ´t que mÃ©diane/moyenne ?**

- KNN capture les **relations locales** entre variables
- MÃ©diane = imputation naÃ¯ve (ignore corrÃ©lations)

**Exemple :** Si `age=NaN` mais `height=180cm, weight=80kg`, KNN trouvera les 5 personnes les plus similaires et utilisera leur `age` moyen.

**âš ï¸ Data Leakage Prevention :** `fit()` sur train uniquement, `transform()` sur test

##### **Ã‰tape 6 : Standardisation (Scaling)**

```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)
```

**Formule :**  
$$X_{scaled} = \frac{X - \mu}{\sigma}$$

**Pourquoi standardiser ?**

- **Arbres de dÃ©cision (RLT) :** Techniquement pas nÃ©cessaire (splits invariants Ã  l'Ã©chelle)
- **Mais utile pour :**
  - Comparaison avec modÃ¨les linÃ©aires (Lasso, Ridge)
  - StabilitÃ© numÃ©rique des calculs de distance (si on ajoute des mÃ©thodes basÃ©es distance)
  - InterprÃ©tabilitÃ© des coefficients (Linear Combination Splits dans RLT)

**âš ï¸ Data Leakage Prevention :** Utiliser `Î¼` et `Ïƒ` du **train** sur le **test**

#### ğŸ“Š **Sortie Console**

```python
print(f"Shape of X_train: {X_train_scaled.shape}")  # Ex: (455, 30)
print(f"Shape of X_test: {X_test_scaled.shape}")    # Ex: (114, 30)
print(f"Shape of y_train: {y_train.shape}")         # Ex: (455,)
print(f"Shape of y_test: {y_test.shape}")           # Ex: (114,)
```

#### ğŸ¯ **Retour de Fonction**

```python
return X_train_scaled, X_test_scaled, y_train, y_test
```

Format compatible avec **scikit-learn** et futures implÃ©mentations RLT

---

## ğŸ”„ Pipeline Complet dans le Notebook

### Phase 2 : Data Understanding (Cellule 11)

```python
for dataset_name in dataset_wrapper.datasets_dict.keys():
    wrapped_ds = dataset_wrapper.DatasetWrapper(dataset_name)
    data_understanding.understand_data(wrapped_ds)
```

**Ce qui se passe :**

1. Boucle sur les 7 datasets configurÃ©s
2. Pour chaque dataset :
   - Chargement via `DatasetWrapper` (normalisation)
   - GÃ©nÃ©ration du rapport complet (7 analyses + visualisations)
3. Sortie : ~50 graphiques + statistiques

### Phase 3 : Data Preparation (Cellule 13)

```python
for dataset_name in dataset_wrapper.datasets_dict.keys():
    wrapped_ds = dataset_wrapper.DatasetWrapper(dataset_name)
    _ = data_preparation.prepare_data(wrapped_ds)
```

**Ce qui se passe :**

1. MÃªme boucle sur 7 datasets
2. Pour chaque dataset :
   - Pipeline de nettoyage (6 Ã©tapes)
   - Affichage des shapes finales
3. Sortie : 7 Ã— 4 matrices (X_train, X_test, y_train, y_test)

**Note :** `_ =` signifie qu'on n'utilise pas encore les donnÃ©es (juste validation)

---

## ğŸ§  DSO1 : ImplÃ©mentation RLT (Phase 4)

### ğŸ¯ Objectif de DSO1

**Reproduire** les expÃ©riences du paper original sur **4 scÃ©narios simulÃ©s** :

| ScÃ©nario | Type           | Variables Signal      | Variables Bruit | ParticularitÃ©                  |
| -------- | -------------- | --------------------- | --------------- | ------------------------------ |
| **1**    | Classification | Xâ‚, Xâ‚‚                | p-2 (indÃ©p.)    | LinÃ©aire simple                |
| **2**    | Classification | Xâ‚, Xâ‚‚                | p-2 (indÃ©p.)    | **Non-linÃ©aire** (sin, exp)    |
| **3**    | Classification | Xâ‚…â‚€, Xâ‚â‚€â‚€, Xâ‚â‚…â‚€, Xâ‚‚â‚€â‚€ | p-4 (corrÃ©lÃ©s)  | **Checkerboard** (interaction) |
| **4**    | RÃ©gression     | Xâ‚…â‚€, Xâ‚â‚€â‚€, Xâ‚â‚…â‚€       | p-3 (corrÃ©lÃ©s)  | LinÃ©aire avec corrÃ©lation      |

**Tests :** p âˆˆ {200, 500, 1000} pour chaque scÃ©nario

### ğŸ”§ Ce qu'il Faut ImplÃ©menter

#### **1. GÃ©nÃ©rateur de DonnÃ©es SynthÃ©tiques**

```python
def generate_scenario(scenario_id, n=1000, p=500, random_state=42):
    """
    GÃ©nÃ¨re un dataset selon les spÃ©cifications du paper.

    Args:
        scenario_id: 1, 2, 3, ou 4
        n: nombre d'Ã©chantillons
        p: dimension totale
        random_state: reproductibilitÃ©

    Returns:
        X: (n, p) array
        y: (n,) array (labels ou valeurs continues)
    """
    rng = np.random.default_rng(random_state)

    if scenario_id == 1:
        # Signal linÃ©aire sur Xâ‚, Xâ‚‚
        X = rng.normal(size=(n, p))
        signal = X[:, 0] + X[:, 1]
        y = (signal > np.median(signal)).astype(int)

    elif scenario_id == 2:
        # Signal non-linÃ©aire
        X = rng.normal(size=(n, p))
        signal = np.sin(X[:, 0]) + np.exp(0.1 * X[:, 1])
        y = (signal > np.median(signal)).astype(int)

    elif scenario_id == 3:
        # Checkerboard avec corrÃ©lation
        # CrÃ©er facteurs latents pour corrÃ©lation
        latent = rng.normal(size=(n, 10))
        loadings = rng.normal(size=(10, p))
        X = latent @ loadings + rng.normal(scale=0.1, size=(n, p))

        # Signal d'interaction (checkerboard)
        signal = (X[:, 49] > 0) != (X[:, 99] > 0)  # XOR pattern
        y = signal.astype(int)

    elif scenario_id == 4:
        # RÃ©gression linÃ©aire avec corrÃ©lation
        latent = rng.normal(size=(n, 10))
        loadings = rng.normal(size=(10, p))
        X = latent @ loadings + rng.normal(scale=0.1, size=(n, p))

        # Signal linÃ©aire
        y = X[:, 49] + X[:, 99] + X[:, 149] + rng.normal(scale=0.5, size=n)

    return X, y
```

#### **2. Classe RLT (Structure de Base)**

```python
class RLT:
    def __init__(self,
                 n_trees=100,              # M dans le paper
                 min_samples_leaf=None,    # nmin = n^(1/3)
                 muting_rate=0.5,          # p_d (0, 0.5, 0.8)
                 k_linear_comb=1,          # k pour linear combination (1, 2, 5)
                 embedded_model='ET',      # Extremely Randomized Trees
                 random_state=None):

        self.n_trees = n_trees
        self.min_samples_leaf = min_samples_leaf
        self.muting_rate = muting_rate
        self.k_linear_comb = k_linear_comb
        self.embedded_model = embedded_model
        self.random_state = random_state
        self.trees_ = []  # Liste d'arbres entraÃ®nÃ©s

    def fit(self, X, y):
        """EntraÃ®ner M arbres avec les 3 stratÃ©gies RLT"""
        n, p = X.shape

        # Calcul nmin si non fourni
        if self.min_samples_leaf is None:
            self.min_samples_leaf = int(n ** (1/3))

        for tree_idx in range(self.n_trees):
            # Bootstrap sample
            boot_indices = np.random.choice(n, size=n, replace=True)
            X_boot, y_boot = X[boot_indices], y[boot_indices]

            # Construire arbre avec stratÃ©gies RLT
            tree = self._build_tree(X_boot, y_boot, depth=0, muted_vars=set())
            self.trees_.append(tree)

        return self

    def _build_tree(self, X, y, depth, muted_vars):
        """RÃ©cursion pour construire un arbre RLT"""
        n, p = X.shape

        # Condition d'arrÃªt
        if n < self.min_samples_leaf or len(np.unique(y)) == 1:
            return {'type': 'leaf', 'value': np.mean(y)}

        # === STRATÃ‰GIE 1 : REINFORCEMENT LEARNING (VI calculation) ===
        active_vars = [i for i in range(p) if i not in muted_vars]
        vi_scores = self._calculate_VI(X[:, active_vars], y)

        # === STRATÃ‰GIE 3 : LINEAR COMBINATION SPLITS ===
        if self.k_linear_comb > 1:
            # SÃ©lectionner top-k variables
            top_k_indices = np.argsort(vi_scores)[-self.k_linear_comb:]
            # CrÃ©er split linÃ©aire : Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... > 0
            split_var, split_val = self._linear_combination_split(
                X[:, active_vars[top_k_indices]], y, vi_scores[top_k_indices]
            )
        else:
            # Split classique sur 1 variable
            split_var = active_vars[np.argmax(vi_scores)]
            split_val = np.median(X[:, split_var])

        # Diviser les donnÃ©es
        left_mask = X[:, split_var] <= split_val

        # === STRATÃ‰GIE 2 : PROGRESSIVE MUTING ===
        # Muter les variables avec faible VI
        threshold_vi = np.quantile(vi_scores, self.muting_rate)
        newly_muted = {active_vars[i] for i, vi in enumerate(vi_scores) if vi < threshold_vi}
        muted_vars_left = muted_vars.union(newly_muted)

        # RÃ©cursion
        left_child = self._build_tree(X[left_mask], y[left_mask], depth+1, muted_vars_left)
        right_child = self._build_tree(X[~left_mask], y[~left_mask], depth+1, muted_vars_left)

        return {
            'type': 'split',
            'var': split_var,
            'val': split_val,
            'left': left_child,
            'right': right_child
        }

    def _calculate_VI(self, X, y):
        """Calculer Variable Importance via embedded model (ET)"""
        from sklearn.ensemble import ExtraTreesClassifier

        # EntraÃ®ner petit modÃ¨le embarquÃ©
        et = ExtraTreesClassifier(n_estimators=10, max_depth=5, random_state=42)
        et.fit(X, y)

        # VI = feature_importances_ de ExtraTrees
        return et.feature_importances_

    def _linear_combination_split(self, X_top_k, y, vi_scores):
        """CrÃ©er split de type Î²â‚Xâ‚ + ... + Î²â‚–Xâ‚– > threshold"""
        # Î²_j = sign(corr(X_j, y)) Ã— VI(j)
        beta = np.array([
            np.sign(np.corrcoef(X_top_k[:, j], y)[0, 1]) * vi_scores[j]
            for j in range(X_top_k.shape[1])
        ])

        # Calculer scores linÃ©aires
        linear_scores = X_top_k @ beta
        threshold = np.median(linear_scores)

        return linear_scores, threshold  # Simplification (retourner scores)

    def predict(self, X):
        """PrÃ©diction par vote majoritaire (classification) ou moyenne (rÃ©gression)"""
        predictions = np.array([self._predict_tree(tree, X) for tree in self.trees_])

        # Vote majoritaire si classification, moyenne si rÃ©gression
        if len(np.unique(predictions)) <= 10:  # Heuristique
            return np.round(np.mean(predictions, axis=0))
        else:
            return np.mean(predictions, axis=0)

    def _predict_tree(self, tree, X):
        """PrÃ©diction pour un arbre unique (rÃ©cursif)"""
        if tree['type'] == 'leaf':
            return np.full(X.shape[0], tree['value'])

        left_mask = X[:, tree['var']] <= tree['val']
        predictions = np.empty(X.shape[0])

        if left_mask.any():
            predictions[left_mask] = self._predict_tree(tree['left'], X[left_mask])
        if (~left_mask).any():
            predictions[~left_mask] = self._predict_tree(tree['right'], X[~left_mask])

        return predictions
```

#### **3. Script d'ExpÃ©rimentation**

```python
# Dans notebook Phase 4 - DSO1

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

results = []

for scenario_id in [1, 2, 3, 4]:
    for p in [200, 500, 1000]:
        # GÃ©nÃ©rer donnÃ©es
        X, y = generate_scenario(scenario_id, n=1000, p=p, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

        # === RLT avec diffÃ©rentes configs ===
        for muting in [0, 0.5, 0.8]:
            for k in [1, 2, 5]:
                rlt = RLT(n_trees=100, muting_rate=muting, k_linear_comb=k)
                rlt.fit(X_train, y_train)
                y_pred = rlt.predict(X_test)
                acc_rlt = accuracy_score(y_test, y_pred)

                results.append({
                    'scenario': scenario_id,
                    'p': p,
                    'model': f'RLT_muting{muting}_k{k}',
                    'accuracy': acc_rlt
                })

        # === Baseline Random Forest ===
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        y_pred_rf = rf.predict(X_test)
        acc_rf = accuracy_score(y_test, y_pred_rf)

        results.append({
            'scenario': scenario_id,
            'p': p,
            'model': 'RandomForest',
            'accuracy': acc_rf
        })

# Analyser rÃ©sultats
results_df = pd.DataFrame(results)
pivot = results_df.pivot_table(
    index=['scenario', 'p'],
    columns='model',
    values='accuracy'
)
print(pivot)
```

#### **4. RÃ©sultats Attendus (selon Paper)**

**ScÃ©nario 3 (Checkerboard) avec p=1000 :**

- Random Forest : ~50-55% accuracy (comme hasard)
- RLT (k=2, muting=0.5) : ~85-90% accuracy âœ…

**Pourquoi ?** RLT dÃ©tecte l'interaction Xâ‚…â‚€ âŠ• Xâ‚â‚€â‚€ grÃ¢ce au VI + linear combination

---

## ğŸ“ˆ MÃ©triques de Validation

### Pour DSO1 (ScÃ©narios SimulÃ©s)

1. **Accuracy** (classification) ou **MSE** (rÃ©gression)
2. **Comparaison RLT vs RF** : RLT devrait gagner sur scÃ©narios 2-4
3. **SensibilitÃ© Ã  p** : performance RLT stable quand pâ†‘, RF dÃ©gradÃ©

### Pour DSO2 (Datasets RÃ©els)

1. **Accuracy / MSE**
2. **Temps d'entraÃ®nement**
3. **Temps de prÃ©diction**
4. **StabilitÃ© (cross-validation)**

---

## ğŸ“ Concepts ClÃ©s Ã  Retenir

### 1. DatasetWrapper : Le Pattern "Adapter"

**ProblÃ¨me :** 10 datasets â†’ 10 formats diffÃ©rents  
**Solution :** 1 interface unifiÃ©e  
**BÃ©nÃ©fice :** Code rÃ©utilisable, maintenable

### 2. Data Understanding : Le Fondement

**Sans comprendre les donnÃ©es :**

- Impossible de choisir bon preprocessing
- Impossible d'interprÃ©ter rÃ©sultats
- Risque de data leakage

### 3. Data Preparation : Le PrÃ©-requis

**Sans prÃ©paration correcte :**

- ModÃ¨les instables (NaN â†’ crash)
- Biais (data leakage)
- Mauvaise gÃ©nÃ©ralisation

### 4. RLT : L'Innovation

**3 StratÃ©gies complÃ©mentaires :**

1. **VI (Reinforcement)** : Look-ahead pour trouver vrais signaux
2. **Muting** : Ã‰liminer bruit progressivement
3. **Linear Comb** : Splits plus expressifs

---

## ğŸ”® Prochaines Ã‰tapes

### DSO2 : Benchmarking

- ImplÃ©menter RF, GBM, XGBoost avec mÃªmes donnÃ©es
- Comparer 4 mÃ©triques (accuracy, time, memory, stability)

### DSO3 : ExplicabilitÃ©

- SHAP values pour RLT
- Feature importance global
- LIME pour prÃ©dictions locales

### DSO4 : Optimisation

- Hyperparameter tuning (GridSearch)
- ParallÃ©lisation (joblib)
- Optimisation mÃ©moire (sparse arrays)

---

## ğŸ“š RÃ©fÃ©rences

1. **Paper Original RLT :** Zhu, R., Zeng, D., & Kosorok, M. R. (2015). _Reinforcement Learning Trees_
2. **CRISP-DM :** Cross-Industry Standard Process for Data Mining
3. **Scikit-learn :** Documentation pour RandomForestClassifier, StandardScaler, etc.

---

**ğŸ“ Note Finale :** Ce document sera mis Ã  jour au fur et Ã  mesure de l'avancement du projet. Chaque modification majeure du code devrait Ãªtre reflÃ©tÃ©e ici.
